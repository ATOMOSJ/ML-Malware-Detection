import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, f1_score
from tabulate import tabulate
import xgboost as xgb

# Importing the dataset
dataset = pd.read_csv('data.csv', sep='|')

# Encode categorical variables
X_encoded = pd.get_dummies(dataset.drop(['Name', 'md5', 'legitimate'], axis=1), drop_first=True)
y = dataset['legitimate']

# Convert labels to binary format
label_encoder = LabelEncoder()
y_binary = label_encoder.fit_transform(y)

# Ensure binary labels
y_binary = np.where(y_binary > 0, 1, 0)

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_binary, test_size=0.20, random_state=0)

# Feature Scaling
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# Feature Selection
fsel = ExtraTreesClassifier().fit(X_train_scaled, y_train)
model = SelectFromModel(fsel, prefit=True)
X_train_selected = model.transform(X_train_scaled)
X_test_selected = model.transform(X_test_scaled)

# K-Nearest Neighbors (K-NN)
classifier_knn = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=2)
classifier_knn.fit(X_train_selected, y_train)
y_pred_knn = classifier_knn.predict(X_test_selected)
cm_knn = confusion_matrix(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)

# Random Forest
classifier_rf = RandomForestClassifier(n_estimators=50, criterion='entropy')
classifier_rf.fit(X_train_selected, y_train)
y_pred_rf = classifier_rf.predict(X_test_selected)
cm_rf = confusion_matrix(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

# XGBoost
dtrain = xgb.DMatrix(X_train_selected, label=y_train)
dtest = xgb.DMatrix(X_test_selected, label=y_test)
param = {'max_depth': 3, 'eta': 0.1, 'objective': 'binary:logistic', 'eval_metric': 'error'}
num_round = 100
bst = xgb.train(param, dtrain, num_round)
y_pred_xgb = bst.predict(dtest)
y_pred_xgb_binary = [1 if y > 0.5 else 0 for y in y_pred_xgb]
cm_xgb = confusion_matrix(y_test, y_pred_xgb_binary)
f1_xgb = f1_score(y_test, y_pred_xgb_binary)

# Display results in tabular format
headers = ["Metric", "K-Nearest Neighbors", "Random Forest", "XGBoost"]
data = [
    ["True Negative (TN)", cm_knn[1, 1], cm_rf[1, 1], cm_xgb[1, 1]],
    ["True Positive (TP)", cm_knn[0, 0], cm_rf[0, 0], cm_xgb[0, 0]],
    ["False Positive (FP)", cm_knn[0, 1], cm_rf[0, 1], cm_xgb[0, 1]],
    ["False Negative (FN)", cm_knn[1, 0], cm_rf[1, 0], cm_xgb[1, 0]],
    ["F1-score", f1_knn, f1_rf, f1_xgb]
]
print(tabulate(data, headers=headers, tablefmt="grid"))
